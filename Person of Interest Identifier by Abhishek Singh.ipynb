{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                     **Person of Interest Identifier**\n",
    "This project is based on the [Enron scandal](https://en.wikipedia.org/wiki/Enron_scandal) of 2001, the data used here is the \n",
    "email and financial data of enron which was made public and can be found [here](https://www.cs.cmu.edu/~./enron/enron_mail_20150507.tar.gz).\n",
    "Here we identify people from the numerous enron employees which can be considered\n",
    "as 'person of interest (poi)' i.e. who may have a hand in the scandal. The data has\n",
    "handpicked people classified as poi which were convicted in reality. We use a supervised\n",
    "learning approach to build our poi identifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We begin by first importing the required packages. Here we mainly work with the scikit-learn package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhis\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data,test_classifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_project_dataset.pkl\", \"rb\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initially we start with all the features in our data however these may contain noise and give a poor accuracy which will be optimized later. Further we extract the features and labels from this data for analysis. Finally we split these in training and testing data for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145, 15)\n"
     ]
    }
   ],
   "source": [
    "features_list = ['poi','salary', 'deferral_payments', 'total_payments', \n",
    "                 'loan_advances', 'bonus', 'restricted_stock_deferred', \n",
    "                 'deferred_income', 'total_stock_value', 'expenses', \n",
    "                 'exercised_stock_options', 'other', 'long_term_incentive', \n",
    "                 'restricted_stock', 'director_fees'] \n",
    "\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "print(data.shape)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To get an initial look at our validation figures we use the simple Naive Bayes classifier without any preprocessing.  We use a test_classifier function which evaluates and displays evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.37753\tPrecision: 0.15641\tRecall: 0.83500\tF1: 0.26347\tF2: 0.44707\n",
      "\tTotal predictions: 15000\tTrue positives: 1670\tFalse positives: 9007\tFalse negatives:  330\tTrue negatives: 3993\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(features_train,labels_train)\n",
    "test_classifier(clf,data_dict,features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see we get a fairly low accuracy and recall score owing to the many features contributing as noise. Our first step towards improving these would be using Principal Component Analysis in which the components with maximum variance can be selected while others can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145, 15)\n",
      "(145, 12)\n"
     ]
    }
   ],
   "source": [
    "num = 12 #This can be tuned to select different number of principal components\n",
    "print(data.shape)\n",
    "pca = PCA(n_components = num)\n",
    "pca.fit(data)\n",
    "dat = pca.transform(data)\n",
    "print(dat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To further augment our selection we use feature selection. Here we use SelectKBest which selects the 'K' best features which explain the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options']\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore') #so that warnings aren't printed\n",
    "\n",
    "labels, features = targetFeatureSplit(dat)\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "selector = SelectKBest(k=7) #To limit the number of features to seven (i.e. approximately half)\n",
    "features_train = selector.fit_transform(features_train,labels_train)\n",
    "\n",
    "mask = selector.get_support() \n",
    "new_features = [] \n",
    "#Here we printout our K best features\n",
    "for bool, feature in zip(mask, features_list):\n",
    "    if bool:\n",
    "        new_features.append(feature)\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other reasons for poor performance can be Outliers present in the data. This can be investigated by using visualization of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"final_project_dataset.pkl\", \"rb\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "features_list = ['bonus','salary'] #Here we choose bonus & salary for visualization\n",
    "data = featureFormat(data_dict,features_list)\n",
    "\n",
    "for point in data:\n",
    "    bonus = point[0]\n",
    "    salary = point[1]\n",
    "    plt.scatter(bonus,salary)\n",
    "plt.xlabel('Bonus')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see the presence of a clear outlier in the data which has been hampering our results. We remove this from our original data and proceed with our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outlier present is  TOTAL\n"
     ]
    }
   ],
   "source": [
    "#To detect the outlier:\n",
    "for i in data_dict:\n",
    "    if (data_dict[i]['salary'] != \"NaN\") & (data_dict[i]['bonus'] != \"NaN\"):    \n",
    "        if (data_dict[i]['salary'] >= 5000000) & (data_dict[i]['bonus'] >= 5000000):\n",
    "            print(\"Outlier present is \",i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The outlier was actually the total row with total figures for all the enron employees. We now remove this for further analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_project_dataset.pkl\", \"rb\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "data_dict.pop('TOTAL') #Outlier removal\n",
    "my_dataset = data_dict\n",
    "features_list = ['poi','loan_advances', 'bonus', 'restricted_stock_deferred', \n",
    "                 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options'] #New feature list\n",
    "\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next classifier we intend to try is SVM. However, SVMs in general are affected by different ranges of the features and may result in output being dominated by a single feature. To overcome this issue we perform feature scaling, in which we first scale our features to an uniform scale of [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scaling =  -126027.0\n",
      "After scaling =  0.09634567351381695\n"
     ]
    }
   ],
   "source": [
    "print('Before scaling = ',data[0,3])\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "scaled_data = scaler.transform(data)\n",
    "print('After scaling = ',scaled_data[0,3]) #Example showing feature scaling effect\n",
    "labels, features = targetFeatureSplit(scaled_data)\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we build our SVM classifier and train it on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a divide by zero when trying out: SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "clf = SVC()\n",
    "clf.fit(features_train,labels_train)\n",
    "test_classifier(clf,data_dict,features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The result from the SVM classifier show that it doesn't perform well here as the number of true positives is very less. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we try out the Random Forest algorithm with a different approach. Here we use GridSearch technique to find parameters which yield the best performance for our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'log2', 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "labels, features = targetFeatureSplit(data)\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "parameters = {'n_estimators':[10,50,100], 'max_features':('auto','log2')}\n",
    "clf = GridSearchCV(rfc,parameters)\n",
    "clf.fit(features_train,labels_train)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above parameters are tested seperately below as test_classifier doesn't work well with GridSearchCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\tAccuracy: 0.87021\tPrecision: 0.60292\tRecall: 0.26800\tF1: 0.37106\tF2: 0.30150\n",
      "\tTotal predictions: 14000\tTrue positives:  536\tFalse positives:  353\tFalse negatives: 1464\tTrue negatives: 11647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50,max_features='log2')\n",
    "clf.fit(features_train,labels_train)\n",
    "test_classifier(clf,data_dict,features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The result shows that this classifier performs fairly well, which can be clearly seen from the above scores. A huge reason for this is here we perform parameter tunning by testing different parameters and selecting the best ones. Changing parameters changes the performance of the classifier and can have impactful effects on its performance.\n",
    "#### Speaking about the evaluation metrics, creating a classifier has no meaning until its performace is tested on data other than the data on which it was trained. This is called as validation, the train_test_split we have been performing, splits the data for training and testing for this purpose.\n",
    "#### We obviously cannot rely on accuracy as a sole measure as it can be doctored easily by skewness present in our data. Hence we use a variety of metrics with accuracy to evaluate our classifier. The precision shows how well the classifier predicts a person of interest i.e. how well the classifer identifies a person as a poi and it is correct. On the other hand recall shows the fraction of total true cases, i.e. the probability of the classifier detecting a poi provided it is actually a poi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, another approach that is often used is creating new features out of the existing features that may help contribute towards the classification. Looking at our features, here we design a new feature called 'savings' which will be the net amout with a person including their salary and bonus and after excluding their expenses, this residual amount with a person can help in identifying their financial status which in this sense could mean their involvement in the fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_project_dataset.pkl\", \"rb\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "#New feature 'savings' created as follows\n",
    "for key,i in data_dict.items():\n",
    "    if i['salary']=='NaN':\n",
    "        i['salary']=0\n",
    "    if i['bonus']=='NaN':\n",
    "        i['bonus']=0\n",
    "    if i['expenses']=='NaN':\n",
    "        i['expenses']=0\n",
    "    i['savings'] = i['salary']+i['bonus']-i['expenses']\n",
    "data_dict.pop('TOTAL') #Outlier removal\n",
    "my_dataset = data_dict\n",
    "#Our feature list this time will contain our new feature which will replace the two features it utilized \n",
    "features_list = ['poi','loan_advances', 'restricted_stock_deferred', \n",
    "                 'deferred_income', 'total_stock_value', 'savings', \n",
    "                 'exercised_stock_options'] \n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We next validate this new feature with our classifier and observe minor improvemets in accuracy. \n",
    "#### However when using our new feature list with the earlier Naive Bayes classifier, improvements in classification are clearly visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='log2', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\tAccuracy: 0.87280\tPrecision: 0.54684\tRecall: 0.26850\tF1: 0.36016\tF2: 0.29893\n",
      "\tTotal predictions: 15000\tTrue positives:  537\tFalse positives:  445\tFalse negatives: 1463\tTrue negatives: 12555\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50,max_features='log2')\n",
    "clf.fit(features_train,labels_train)\n",
    "test_classifier(clf,data_dict,features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.77387\tPrecision: 0.28236\tRecall: 0.45150\tF1: 0.34744\tF2: 0.40320\n",
      "\tTotal predictions: 15000\tTrue positives:  903\tFalse positives: 2295\tFalse negatives: 1097\tTrue negatives: 10705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "clf.fit(features_train,labels_train)\n",
    "test_classifier(clf,data_dict,features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thus we have performed analysis on the Enron dataset classifying the employees as person of interests. Another approach which can be utilized for the same is performing text learning on the text data i.e. Enron email data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
